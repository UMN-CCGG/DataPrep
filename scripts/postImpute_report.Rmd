---
title: "Post-Imputation Report"
author: Patrick Monnahan
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    includes:
      in_header: header.tex
params:
  chrom_file: NA
  merge_file: NA
  chunk_file: NA
  snp_file: NA
  rulegraph_file: NA
  config_file: NA
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir=normalizePath('../'))
knitr::opts_chunk$set(echo = FALSE, fig.height = 6, fig.width = 6, fig.align = 'center', fig.pos = 'H')
```

```{r dependencies, include=FALSE}
library("ggplot2")
library("reshape2")
library("optparse")
library("yaml")
library("dplyr")
library("tidyr")
library("stringr")
library("magrittr")
```

```{r read_params, include=FALSE}
if(any(unlist(lapply(params, function(x) x=="NA")))) stop("Missing input params")
chrom_file <- read.table(params$chrom_file)
merge_file <- read.table(params$merge_file)
chunk_file <- read.table(params$chunk_file, head = T)
snp_file <- read.table(params$snp_file, head=T)
rulegraph_file <- params$rulegraph_file
yaml <- read_yaml(params$config_file)

```

This report contains summary information of the process that was used to convert imputed data from the \href{https://imputation.biodatacatalyst.nhlbi.nih.gov/#!}{{\color{blue}{\underline{TOPMed Imputation Server}}}} into a PLINK-formatted dataset that is ready for association analysis or admixture inference.  In brief, one or more sets of gzipped VCF files are first run through CrossMap, which converts coordinates from the GRCh38 reference genome to GRCh19.  Then, these files are converted to PLINK format and variants are filtered for missingness, duplicates, and indels (are removed).  For each chromosome, we then merge these resulting files across datasets.  Only variants that have been retained across all datasets are included in this merged dataset.  Rare alleles are then filtered from this merged dataset.  The DAG representing this workflow is provided at the end of this document, although it may be difficult to view.  Also, see the config.yml in the workflow directory for full list of parameter inputs and settings.

The following datasets were used as input:

```{r input-datasets, warning=FALSE, message=FALSE}
yaml$query %>% as.data.frame() %>% gather("Dataset", "Directory") %>% knitr::kable()
```

and the pipeline was carried out using the following singularity image:
```{r singularity-image}
yaml$singularity$image
```

# Imputation Summary
The TOPMed Imputation Server is based on the Michigan Imputation Server technology and thus implements a series of filters on the input datasets (see \href{https://imputationserver.sph.umich.edu/index.html#!pages/pipeline}{{\color{blue}{\underline{here}}}} for details) prior to imputation.  

## Input Filtering

### Variant Exclusion
For each variant, the server will attempt to convert coordinates to hg38 (if necessary) via the LiftOver tool.  If this is successful, it will determine if the variant matches a variant in the reference dataset along with whether the alleles in the query dataset match those in the reference.  If a mismatch is found, the type of mismatch (flip, swap, or flip+swap) is determined and the variant is removed.  The plot below summarizes the number of variants that were excluded subdivided by reason for exclusion.  Note that these unstandardized totals will likely depend heavily on the total number of variants in the input query dataset. 
```{r excluded-snps, fig.height = 4, fig.width = 4, warning=FALSE, message=FALSE}
snp_file %>% group_by(dataset,filter) %>% summarize(n=n()) %>% ggplot(aes(x=filter,y=n,fill=dataset)) + geom_bar(stat="identity",position=position_dodge()) + xlab("") + theme(axis.text.x = element_text(angle=25,hjust=1)) + ylab("Number of Filtered Variants")
```

### Chunk Exclusion
The chromosomes are then divided into 20Mb chunks, and these chunks are excluded from imputation if: 1.) there are fewer than 3 SNPs, 2.) if <50% of sites in query dataset are found in reference dataset, 3.) any samples has a callrate <50%. 

Below is the full list of excluded chunks along with the number of datasets that they were excluded from and the reasons for exclusion.
```{r excluded-chunks, warning=FALSE, message=FALSE}

chunk_file %<>% mutate(low.snp.number=case_when(num.snps<3~1,TRUE~0), low.ref.ovlp=case_when(ref.ovlp<0.5~1,TRUE~0),bad.samp=case_when(num.low.sample.callrate>0~1,TRUE~0))

chunk_file %>% group_by(chunk) %>% summarize(Low.SNP.Number = sum(low.snp.number), Low.Ref.Overlap = sum(low.ref.ovlp), Bad.Sample = sum(bad.samp), Num.DataSets = n()) %>% knitr::kable()
```

Experience has shown that there should only be a few excluded chunks, which tend to be relatively consistent across datasets (e.g. chromosome 9 and 14)

Number of excluded chunks in each dataset, further classified by the reason for which they were filtered. Note that a single chunk may have failed multiple filters.
```{r dataset-chunks, warning=FALSE, message=FALSE}
chunk_file %>% group_by(dataset) %>% summarize(Low.SNP.Number = sum(low.snp.number), Low.Ref.Overlap = sum(low.ref.ovlp), Bad.Sample = sum(bad.samp), Total = n()) %>% knitr::kable()
```

\newpage

The plot below shows the number of imputed variants after excluding the SNPs and chunks listed above.  If few chunks were excluded, then these numbers should be very consistent across datasets.  Note that numbers are slightly artificially inflated due to the coding of multiallelic variants.  These are represented on multiple lines, one for each alternative allele.  These multiallelic variants as well as the indels are removed, subsequently, and generally make up a small portion (5-6%) of the total number.  Also, note that the majority of these imputed variants are likely fixed for one allele and will ultimately be removed (see 'Removing rare alleles' below)
```{r chrom-total-imputed, fig.cap="Total number of imputed variants (SNPs and Indels)", fig.height = 6, fig.width = 7, warning=FALSE, message=FALSE}
totals = chrom_file %>% filter(V3=="raw") %>% mutate(Chromosome=as.ordered(as.numeric(str_remove(V2, "chr"))))

totals %>% ggplot(aes(y=V4/1000000,x=Chromosome,fill=Chromosome)) + geom_bar(stat="identity",position=position_dodge()) + ylab("Millions of Variants") + xlab("Chromosome") + facet_wrap(~V1) + theme(axis.text.x=element_text(angle=45,hjust=1), legend.position = "None")
```

\newpage

# Coordinate Conversion
The program \href{http://crossmap.sourceforge.net/}{{\color{blue}{\underline{CrossMap}}}} was used to convert coordinates from GRCh38 to GRCh19.  The reference fasta and 'chain' files (key linking coordinates across chromosomes) were taken from:

Reference Fasta
```{r ref-fasta}
yaml$CrossMap$fasta
```

Chain File
```{r chain-file}
yaml$CrossMap$chain
```

```{r crossmap-unmapped, fig.cap="Proportion of variants whose coordinates were not successfully cross-mapped",fig.height = 6, fig.width = 7, warning=FALSE, message=FALSE}
chrom_file %>% filter(V3 %in% c("unmap","raw")) %>% pivot_wider(id_cols=c("V1","V2"),names_from="V3",values_from="V4") %>% mutate(Dataset=V1,Chromosome=as.ordered(as.numeric(str_remove(V2, "chr"))),Unmapped=unmap / (unmap + raw)) %>% select(c(Dataset,Chromosome,Unmapped)) %>% pivot_longer(c(Unmapped),names_to = "State", values_to = "Variants") %>% ggplot(aes(x=Chromosome,fill=Dataset,y=Variants)) + geom_bar(stat="identity", position=position_dodge(), width=0.75) + ylab("Proportion of unmapped variants") + theme(axis.text.x=element_text(angle=45,hjust=1))
```
The proportion here is calculated for each chromosome as:  #unmapped / (#mapped + #unmapped). Unmapped variants were removed from subsequent steps.
\newpage

# PLINK Conversion and initial QC
Following coordinate conversion, the VCFs for each chromosome are converted to plink format.  During this conversion, poorly imputed genotypes are filtered out.  That is, if the probability of the most probable genotype falls below the following threshold, then the genotype for this sample is set to missing.

```{r min-genotype-prob}
yaml$min_gp
```
Thus, the missingness filter discussed below also filters for imputation 'quality'.

Variants are then filtered for missingness and duplicates and indels are removed.  The threshold for maximum proportion of missing samples for a given variant is:

```{r vm2}
yaml$QC$vm2
```

This missingness criterion is applied after first excluding samples that exceeded the following rate of missingness across variants:

```{r gm}
yaml$QC$gm
```

<!-- \newpage -->
<!-- \blandscape -->

```{r chrom-filter-stats, fig.cap = "Proportion of total imputed sites removed by each filter.", fig.height = 5.5, fig.width = 7, warning=FALSE, message=FALSE}

chrom_file %>% filter(!V3 %in% c("frq","hwe")) %>% pivot_wider(id_cols=c("V1","V2"),names_from="V3",values_from="V4") %>% mutate(Dataset=V1,Chromosome=V2,Missingness=lmiss/raw,Total=txt/raw,Duplicates=dupvar/raw,Indels = (txt - lmiss - dupvar)/raw) %>% select(c(Dataset,Chromosome,Missingness,Duplicates,Indels)) %>% pivot_longer(c(Missingness,Duplicates,Indels),names_to = "Filter", values_to = "Proportion") %>% ggplot(aes(x=Dataset,fill=Filter,y=Proportion)) + geom_bar(stat="identity") + facet_wrap(~Chromosome) + theme(axis.text.x=element_text(angle=45,hjust=1))
```
Note: the proportion of variants removed due to missingness may be appear artificially small here.  Nonvariant sites, which make up vast majority of imputed variants tend to be imputed with high probability, which translates to low missingness.  Indels includes multiallelic variants as well, which are the vast minority. 

<!-- \elandscape -->
\newpage

# Filtering Merged Data

## Overlap filtering
If multiple imputed datasets were provided as input, the next step would be, for each chromosome, to merge the genotypes across datasets.  Importantly, only variants that are still present in all datasets (i.e. have not been filtered in any single dataset) will be retained.  This way, if a variant imputed poorly in one dataset for whatever reason, it would be removed entirely from the merged dataset.  

```{r merge-stats, fig.cap = "Proportion of variants found in all component datasets for each chromosome", fig.height = 3, fig.width = 5, warning=FALSE, message=FALSE}

Totals = totals %>% group_by(Chromosome) %>% summarize(total = max(V4,na.rm=T))

merge_file %<>% mutate(Chromosome=as.ordered(as.numeric(str_remove(V1, "chr")))) %>% left_join(Totals, by="Chromosome") %>% mutate(Proportion = V3/total)
merge_file %<>% mutate(prop = V3/total)

merge_file %>% filter(V2=="ovlp") %>% ggplot(aes(x=Chromosome,y=prop, fill = Chromosome)) + geom_bar(stat="identity",position=position_dodge()) + ylab("Proportion overlapping variants") + scale_fill_discrete(guide=F)
```

\newpage
## Removing rare alleles 

Removal of rare alleles is the final and likely most consequential step in the post-imputation QC pipeline.  We wait to filter rare alleles until after merging in case there are fixed differences across datasets.  Such variants, although rare in each individual dataset, may be intermediate in the merged dataset.  The reason that this filter will likely remove the largest number of variants is due to the fact that the majority of imputed variants are, in fact, non-variant.  The 
```{r maf-stats, fig.cap = c("Proportion of variants that were RETAINED following removal of rare SNPs", "Total number of imputed variants remaining in the final QC'ed dataset."), fig.height = 3, fig.width = 5, warning=FALSE, message=FALSE}

merge_file %>% filter(V2=="MAF") %>% ggplot(aes(x=Chromosome,y=prop, fill = Chromosome)) + geom_bar(stat="identity",position=position_dodge()) + ylab("Proportion of RETAINED variants") + scale_fill_discrete(guide=F)

merge_file %>% filter(V2=="MAF") %>% ggplot(aes(x=Chromosome,y=V3, fill = Chromosome)) + geom_bar(stat="identity",position=position_dodge()) + ylab("Numbers of retained common variants") + scale_fill_discrete(guide=F)
```
\newpage

# Rule Graph

Below is a directed acyclic graph depicting the steps involved in this post-imputation QC pipeline.  When possible, computation within each node was parallelized by dataset, chromosome, etc.  The full DAG visualizing the parallel computing can be generated via:

    snakemake --dag | dot -Tpng > jobgraph.png
    
from within the directory that the post-imputation QC was carried out.  These are typically too large to fit easily in a pdf, and so were not included in this report.


```{r, fig.cap = "A rule graph showing the different steps of the bioinformatic analysis that is included in the Snakemake workflow.", out.height = "11cm"}
knitr::include_graphics(normalizePath(rulegraph_file))
```



\newpage

# Reproducibility

The code for reproducing this analysis is available \href{https://github.com/pmonnahan/DataPrep/tree/master/postImpute/workflow}{{\color{blue}{\underline{here}}}}.  The repo contains:

* A Snakemake workflow for running all steps.
* A collection of scripts to acheive individual steps
* A Singularity definitions file that can be used to generate the Singularity image used to run all steps.
** This image file is also directly available upon request

The code for reproducing this report is available \href{https://github.com/pmonnahan/DataPrep/blob/master/scripts/postImpute_report.Rmd}{{\color{blue}{\underline{here}}}}. 

The input files for the figures produced herein are from:

```{r input-files}
params
```
Also, see the config.yml in the workflow directory for full list of parameter inputs and settings.

The results in this supplementary were generated in the following R environment:

\footnotesize
```{r session_info}
sessionInfo()
```
\normalsize
