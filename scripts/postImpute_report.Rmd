---
title: "Post-Imputation Report"
author: Patrick Monnahan
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    includes:
      in_header: header.tex
params:
  chrom_file: NA
  merge_file: NA
  rulegraph_file: NA
  config_file: NA
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir=normalizePath('../'))
knitr::opts_chunk$set(echo = FALSE, fig.height = 6, fig.width = 6, fig.align = 'center', fig.pos = 'H')
```

```{r dependencies, include=FALSE}
library("ggplot2")
library("reshape2")
library("optparse")
library("yaml")
library("dplyr")
library("tidyr")
library("stringr")
```

```{r read_params, include=FALSE}
if(any(unlist(lapply(params, function(x) x=="NA")))) stop("Missing input params")
chrom_file <- read.table(params$chrom_file)
merge_file <- read.table(params$merge_file)
rulegraph_file <- params$rulegraph_file
yaml <- read_yaml(params$config_file)

```

# Post-Imputation Report

## Preparing samples

This report contains summary information of the process that was used to convert imputed data from the Michigan Imputation Server (likely using the TOPMed imputation panel) into a PLINK-formatted dataset that is ready for association analysis or admixture inference.  In brief, one or more sets of gzipped VCF files are first run through CrossMap, which converts coordinates from the GRCh38 reference genome to GRCh19.  Then, these files are converted to PLINK format and variants are filtered for missingness, duplicates, and indels (are removed).  Within each imputed dataset, variants are then merged across chromosomes, and the resulting files are merged across datasets.  Only variants that have been retained across all datasets are included in this merged dataset.  Rare alleles are then filtered from this merged dataset.  The DAG representing this workflow is provided at the end of this document, although it is likely too large to be conveniently viewed.

The following datasets were used as input:

```{r input-datasets}
yaml$query %>% as.data.frame() %>% gather("Dataset", "Directory") %>% knitr::kable()
```

and the pipeline was carried out using the following singularity image:
```{r singularity-image}
yaml$singularity$image
```

## Chromosome-level filters

Following coordinate conversion, the VCFs for each chromosome are converted to plink format.  Variants are then filtered for missingness and duplicates and indels are removed.  The threshold for maximum proportion of missing samples for a given variant is:

```{r vm2}
yaml$QC$vm2
```

This missingness criterion is applied after first excluding samples that exceeded the following rate of missingness across variants:

```{r gm}
yaml$QC$gm
```

The following figures summarize the number of variants excluded due to each of the filters mentioned above:

\newpage

Total number of imputed variants (SNPs and Indels)
```{r chrom-total-imputed}
chrom_file %>% filter(V3=="raw") %>% mutate(Chromosome=as.ordered(as.numeric(str_remove(V2, "chr")))) %>% ggplot(aes(y=V4/1000000,x=Chromosome,fill=Chromosome)) + geom_bar(stat="identity",position=position_dodge()) + ylab("Millions of Variants") + xlab("Chromosome") + theme(axis.text.x=element_text(angle=45,hjust=1), legend.position = "None")
```

\newpage

Proportion of total imputed sites removed by each filter
```{r chrom-filter-stats}
chrom_file %>% filter(!V3 %in% c("frq","hwe")) %>% pivot_wider(id_cols=c("V1","V2"),names_from="V3",values_from="V4") %>% mutate(Dataset=V1,Chromosome=V2,Missingness=lmiss/raw,Total=txt/raw,Duplicates=dupvar/raw,Indels = (txt - lmiss - dupvar)/raw) %>% select(c(Dataset,Chromosome,Missingness,Duplicates,Indels,Total)) %>% pivot_longer(c(Missingness,Duplicates,Indels,Total),names_to = "Filter", values_to = "Proportion") %>% ggplot(aes(x=Dataset,fill=Filter,y=Proportion)) + geom_bar(stat="identity",position=position_dodge()) + facet_wrap(~Chromosome) + theme(axis.text.x=element_text(angle=45,hjust=1))
```

\newpage

## Rule Graph
```{r, fig.cap = "A rule graph showing the different steps of the bioinformatic analysis that is included in the Snakemake workflow.", out.height = "11cm"}
knitr::include_graphics(normalizePath(rulegraph_file))
```



\newpage

## Reproducibility

The code for reproducing this analysis is available in this [Github repo](https://github.com/pmonnahan/DataPrep/blob/master/scripts/postImpute_report.Rmd). 

The results in this supplementary were generated in the following R environment:

\footnotesize
```{r session_info}
sessionInfo()
```
\normalsize
