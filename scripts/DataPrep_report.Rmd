---
title: "Pre-imputation QC report"
author: Patrick Monnahan
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    includes:
      in_header: header.tex
    toc: true
    toc_depth: 3
params:
  flt_file: NA
  FixRef_file: NA
  rulegraph_file: NA
  config_file: NA
  ref_freqs: NA
  dat_freq: NA
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir=normalizePath('../'))
knitr::opts_chunk$set(echo = FALSE, fig.height = 6, fig.width = 6, fig.align = 'center', fig.pos = 'H')
```

```{r dependencies, include=FALSE}
library("ggplot2")
library("reshape2")
library("yaml")
library("dplyr")
library("tidyr")
library("magrittr")
library("wrapr")
library("stringr")
library("foreach")
library("pander")
```

```{r read_params, include=FALSE}

if(any(unlist(lapply(params, function(x) x=="NA")))) stop("Missing input params")
flt <- read.table(params$flt_file)
fix <- read.table(params$FixRef_file, col.names = paste0("V",seq_len(4)), fill = TRUE)
rulegraph_file <- params$rulegraph_file
ref <- read.table(params$ref_freqs)
ref %<>% mutate(freq=V4,chrom=V1,pos=V2,ID=V3) %>% select(-c(V1,V2,V3,V4))
# dat_freq is composed of multiple datasets with differing numbers of rows.
dat_files = str_split(params$dat_freq, ",")
yaml <- read_yaml(params$config_file)
```

This report contains summary information of the process that was used to filter and prepare different datasets for imputation via the Michigan Imputation Server (likely using the TOPMed imputation panel).  Beginning with one or more datasets in PLINK format, optional updates are made to the chromosome and variant IDs in an attempt to make these uniform across datasets.  Additionally, alleles can be converted with a provided key. Then, a series of steps are performed in an attempt to correctly set the reference versus alternative alleles as implied by a specified reference genome.  With this 'fixed' dataset, we perform a series of basic QC steps described in a subsequent section.  After QC, we restore the reference alleles determined in the previous step as well as any phenotypes or sex specification that was lost.  Finally, the datasets are split by chromosome and converted into separate sorted, gzipped VCF files.  The DAG representing this workflow is provided at the end of this document, although it is likely too large to be conveniently viewed.

The following datasets were used as input:

```{r input-datasets}
yaml$query %>% as.data.frame() %.>% pivot_longer(., colnames(.), names_to = "Names", values_to = "Directory") %>% separate(Names,into = c("Dataset", "key"), "[.]") %>% filter(key=="data") %>% select(-key) %.>% pander::pander(., split.cell = 80, split.table = Inf)
```

and the pipeline was carried out using the following singularity image:
```{r singularity-image}
yaml$singularity$image
```

## Reference allele fixing

In contrast to a VCF, where alleles are specified with respect to a specified reference genome (reference versus alternative alleles), PLINK-formatted files often specify alleles as major/minor alleles based on the frequency in the dataset.  Furthermore, many commonly used arrays will contain a mixture of SNPs genotyped on either the + or - strand.  Lastly, the default behavior of PLINK is to automatically set the minor to A1 and the major allele to A2, which can unintentionally generate inconsistencies in allele specifications across datasets.  

With respect to a reference genome, two possible types of errors can occur:
-   Flipped strand:  The genotype is specified with respect to the opposite strand relative to the reference genome.
-   Swapped allele:  The genotype is specified on the same strand as the reference genome, but the A1 (minor) allele has been set to equal the 'reference' allele when it ought to be set to equal the non-reference/'alternative' allele

To identify these errors, we use the bcftools plugin '+fixref', which requires not only the reference sequence (fasta) file, but also a VCF file containing variant sites that are used to identify mismatching alleles in the query dataset.  Importantly, if the program determines that no strand issues exist and that the reference/alternative alleles have simply been swapped, then program will swap the major/minor alleles to match the reference.  It will not perform any strand flipping, where it converts genotypes to be specified with respect to the nucleotide on the opposite strand.  Although the program will attempt to identify these strand flips, it doesn't make the correction as the authors consider this a risky move that should not be handled in an automated fashion.  Thus, flip-strand mismatches are ultimately removed.  If there are a large number of these, the user should attempt to understand and resolve the source of the issue and rerun this pipeline.

By default, the pipeline will download the following files for the hg19 reference genome:

Reference fasta: 
ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz

Reference VCF (1000Genomes):
ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b150_GRCh37p13/VCF/All_20170710.vcf.gz


### bcftools +fixref Summary:
The four possible allelic states that 'bcftools +fixref' will identify are that the major allele in the query dataset: 1.) matches the reference (ref-match), does not match the reference (ref-mismatch), is non-ACGT (e.g. plink sometimes calls alleles as A/B), is non-SNP (i.e. is an indel), or is non-biallelic
```{r FixRef-StateSummary, fig.cap="Number of variants subdivided by allelic state with respect to the reference genome",fig.height = 5, fig.width = 8, warning=FALSE, message=FALSE}
fix %<>% distinct() %>% group_by(V1,V2) %>% summarize(max_sites = max(V3), percent = max(V4,na.rm=T), min_sites = min(V3)) %>% mutate(Dataset=V1)

fix %>% mutate(sites = case_when(V2=="ref-match" ~ min_sites, V2!="ref-match" ~ max_sites), State=V2) %>% filter(State %in% c("total","ref-match","ref-mismatch","non-ACGT","non-SNP","non-biallelic")) %>% ggplot(aes(x=Dataset, fill = State, y = sites/1000)) + geom_bar(stat="identity",position=position_dodge()) + ylab("Sites (in thousands)")
```
The 'ref-mismatch' sites are the ones in which the program will try to take action to correct.


```{r FixRef-ActionTaken, fig.cap="Number of sites in which the corresponding action was taken", warning=FALSE, message=FALSE}
fix %>% mutate(sites = max_sites, Action=V2) %>% filter(Action %in% c("skipped","swapped","flipped","flip+swap","fixed-pos","unresolved")) %>% ggplot(aes(x=Dataset, fill = Action, y = sites/1000)) + geom_bar(stat="identity",position=position_dodge()) + ylab("Sites (in thousands)")
```
Since bcftools does not do any flipping, anything in the 'flipped' and 'flip+swap' will be removed.  With the exception of the 'unresolved' class (which includes the flipped and flip+swap variants), these 'corrected' sites will be included in the new totals shown below.  The 'unresolved' sites are essentially lost data and are filtered from the final datasets.

```{r FixRef-NewTotals, fig.cap="New total number of sites following the changes made by bcftools +fixref", warning=FALSE, message=FALSE}
fix %>% filter(V2=="total") %>% ggplot(aes(x=1, fill=Dataset, y=min_sites/1000)) + geom_bar(stat="identity",position=position_dodge()) + ylab("Sites (in thousands)") + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank())
```
Ideally, there should be >400k remaining SNPs after this point.  Lower SNP density (<300k total sites) has been associated with poor imputation outcomes.  

An indication of whether alleles are now specified correctly is to plot frequency of an allele in the query population against the frequency in the reference population and look for an obviously positive correlation.  The graphs below provide such plots for each of the query datasets:
```{r AF-correlation, fig.cap="Scatter plots showing correlation between reference allele frequency as measured in the reference VCF and query data", warning=FALSE, message=FALSE}

Dat = foreach(i = 1:length(dat_files[[1]]), .combine=rbind) %do% {
  dat = read.table(dat_files[[1]][i], na.strings=".", fill = T)
  # We do 1 - freq below, so that we get REF frequency as reported in ref_freqs.txt
  #THIS NEXT STEP IS CRASHING RECENT RUNS WITH 32GB OF MEMORY.  It's the pivot call
  dat = dat %>% pivot_longer(-c(V1,V2,V3), names_to="hap",values_to="allele") %>% group_by(V1,V2,V3) %>% summarize(freq = 1 - sum(allele,na.rm=T)/n(), datset=str_split(basename(dat_files[[1]][i]),"[.]")[[1]][1], dd = basename(dat_files[[1]][i]))
  dat %<>% mutate(chrom=V1,pos=V2,ID=V3)
  dat
}

Dat %>% left_join(ref, by=c("chrom","pos","ID")) %>% ggplot(aes(x=freq.x,y=as.numeric(freq.y))) + geom_point() + facet_wrap(~datset,nrow=2) + ylab("Reference AF") + xlab("Query AF")

```


## Dataset QC

After alleles have been fixed as described above, a series of basic QC steps are performed on each dataset.  First, we exclude variants with a rate of missingness exceeding:

```{r vm1}
yaml$QC$vm1
```

Then, among the remaining variants, samples exceeding the following rate of missingness across all variants are excluded:

```{r gm}
yaml$QC$gm
```

Using the remaining samples, we identify and exclude variants with a rate of missingness exceeding:

```{r vm2}
yaml$QC$vm2
```

Then, we remove variants with a minor allele frequency less than:

```{r maf}
yaml$QC$maf
```

Or, variants whose p-value for a test of Hardy-Weinberg Equilibrium is below:

```{r hwe}
yaml$QC$hwe
```

as well as any duplicate variants.

The table below lists the number of variants excluded for each filter, while the proceeding figure presents the same information but shown as a proportion of the total of all filtered sites.  Note that it is possible for a variant to get flagged by multiple filters, such that the sum of the values from individual filters will not equal the total filtered.

```{r chrom-filter-stats, fig.cap="Number of variants excluded per filters in each of the datasets", warning=FALSE, message=FALSE}

flt %<>% pivot_wider(id_cols=c("V1"),names_from="V2",values_from="V3") %>% mutate(Dataset=V1, Missingness=lmiss, MAF=frq, HWE=hwe, Total=txt, Duplicates=txt - frq - hwe - lmiss, Raw=raw) %>% select(c(Dataset,Missingness,HWE,MAF,Total))

fix.tmp = fix %>% filter(V2=="total") %>% ungroup() %>% mutate(Raw = min_sites) %>% select(Dataset, Raw)

flt %<>% left_join(fix.tmp, by="Dataset")

# print table
flt %>% knitr::kable()
```


```{r chrom-filter-breakdown, fig.cap="Proportion of variants excluded by filter (as proportion of total filtered sites)", fig.height = 4, fig.width = 6,warning=FALSE, message=FALSE}

flt %>% mutate(Missingness = Missingness/Total, HWE = HWE/Total, MAF = MAF/Total) %>% select(-c(Total,Raw)) %>% pivot_longer(-Dataset,names_to = "Filter", values_to = "Proportion") %>% ggplot(aes(x=Dataset,fill=Filter,y=Proportion)) + geom_bar(position=position_dodge(), stat="identity")
```


```{r chrom-filter-totals-prop, fig.cap="Total sites filtered as proportion of raw number of sites", warning=FALSE, message=FALSE}

flt %>% select(Dataset, Total,Raw) %>% mutate(Filtered = Total/Raw) %>% select(-c(Total,Raw)) %>% ggplot(aes(x = 1, y=Filtered,fill=Dataset)) + geom_bar(stat = "identity", position=position_dodge()) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()) + ylab("Proportion of sites filtered")
```


```{r chrom-remaining-vars, fig.cap="Total number of remaining variants for each dataset", warning=FALSE, message=FALSE}

flt %>% select(c(Dataset,Total,Raw)) %>% mutate(Filtered=Total, Remaining = Raw - Total) %>% select(Dataset,Raw,Filtered,Remaining) %>% knitr::kable()
```

For imputation to work reasonably well, each dataset should still have >300k variants, and ideally >500k.  Datasets with fewer variants are associated with low genotype probabilities inferred during the imputation process, which may result in the imputed genotype (and perhaps the entire variant) being filtered from the imputation results.  

\newpage

## Rule Graph

Below is a directed acyclic graph depicting the steps involved in this post-imputation QC pipeline.  When possible, computation within each node was parallelized by dataset, chromosome, etc.  The full DAG visualizing the parallel computing can be generated via:

    snakemake --dag | dot -Tpng > jobgraph.png
    
from within the directory that the post-imputation QC was carried out.  These are typically too large to fit easily in a pdf, and so were not included in this report.

```{r, fig.cap = "A rule graph showing the different steps of the bioinformatic analysis that is included in the Snakemake workflow.", out.height = "11cm"}
knitr::include_graphics(normalizePath(rulegraph_file))
```

\newpage

## Reproducibility

The code for reproducing this analysis is available
\href{https://github.com/pmonnahan/DataPrep}{{\color{blue}{\underline{here}}}}. The repo contains:

* A Snakemake workflow for running all steps.
* A collection of scripts to acheive individual steps
* A Singularity definitions file that can be used to generate the Singularity image used to run all steps.
** This image file is also directly available upon request

The code for reproducing this report is available \href{https://github.com/pmonnahan/DataPrep/blob/master/scripts/DataPrep_report.Rmd}{{\color{blue}{\underline{here}}}}.

The input files for the figures produced herein are from:

```{r input-files}
params
```
Also, see the config.yml in the workflow directory for full list of parameter inputs and settings.

The results in this supplementary were generated in the following R environment:

\footnotesize
```{r session_info}
sessionInfo()
```
\normalsize
