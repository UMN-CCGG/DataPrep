# Load modules
import glob
import os
import subprocess
import pdb
import shutil
import yaml

# Get the date
from datetime import datetime
i = datetime.now()
TIME = i.strftime('%Y-%m-%d')

# Specify config file
configfile: "workflow/config.yml"

# Parse config.yml file
OUT = config['outname']
QUERY = config['query']
BCFTOOLS = config['bcftools']['executable']
PYTHON = config['python']

# Is reference info provided in config file or do we download?
if config['build'] == "37":
    REFASTA = config['reference']['refFasta37']
    REFVCF = config['reference']['refVCF37']
if config['build'] == "38":
    REFASTA = config['reference']['refFasta38']
    REFVCF = config['reference']['refVCF38']
if config['FixRef'] == "true":
    if config['reference']['download'] == 'true':
        REFASTA = f"{os.getcwd()}/RefAnnotationData/{os.path.basename(REFASTA)}"
        REFVCF = f"{os.getcwd()}/RefAnnotationData/{os.path.basename(REFVCF)}"
    else:
        assert (REFASTA != "none" and REFVCF != "none"), "Must provide a reference fasta and VCF for build 37 in workflow/config.yml file"
        assert os.path.exists(config['reference']['refFasta37']), f"Did not find reference fasta for build 37: {config['reference']['refFasta37']}"
        assert os.path.exists(config['reference']['refVCF37']), f"Did not find reference VCF for build 37: {config['reference']['refVCF37']}"
        assert os.path.exists(config['reference']['refFasta38']), f"Did not find reference fasta for build 38: {config['reference']['refFasta38']}"
        assert os.path.exists(config['reference']['refVCF38']), f"Did not find reference VCF for build 38: {config['reference']['refVCF38']}"

# Format commands and bind paths if using singularity.
if config['singularity']['use_singularity'] == 'true' and config['singularity']['image'] != "none":
    query_paths = []
    for x in QUERY.keys():
        for k,v in QUERY[x].items():
            path = os.path.dirname(str(v))
            if path:
                if path not in query_paths: query_paths.append(path)
    bind_paths = ",".join(set(query_paths + [os.path.dirname(REFASTA), os.path.dirname(REFVCF)] + [f"{os.getcwd()}/{x}" for x in ['OandE', 'accessory', config['singularity']['code']] + list(QUERY.keys())]))
    CMD_PREFIX = f"set +u; {config['singularity']['module']}; BCFTOOLS_PLUGINS={config['bcftools']['plugins']}; singularity exec --bind {bind_paths},{os.getcwd()} {config['singularity']['image']}"
    CODE = config['singularity']['code']
else:
    CMD_PREFIX = config['cmd_prefix']
    CODE = config['dir']['code']


# Make subdirectories
if not os.path.exists("RefAnnotationData"): os.mkdir("RefAnnotationData")
if not os.path.exists("accessory"): os.mkdir("accessory")
if not os.path.exists("OandE"): os.mkdir("OandE")
for datset in QUERY.keys(): #Make subdirectory for each dataset
    if not os.path.exists(datset):
        os.mkdir(datset)
        os.mkdir(f"{datset}/tmp")

#TODO Make this robust to chromosomes > 23.
CHROMS = config['chroms']
if CHROMS == 'all':
    CHROMS = [str(x) for x in range(1, 23)]
    if config['include_x'] == 'yes': CHROMS.append('X')

# Run locally or submit to cluster depending on config.yml
plink_run_specs = {'__default__': ""} # Set default to empty string in case of local run.  i.e. default to plink's auto-detection of resources.
if config['run_settings']['local_run'] == 'true':
    localrules: all, update_alleles_IDs, FixRef, QCcombine_query, set_pheno
else:
    localrules: all
    assert config['run_settings']['scheduler'] in ['pbs', 'slurm'], print(f"\nThe scheduler specified at run_settings -> scheduler: \'{config['run_settings']['scheduler']}\' is invalid.\n")
    assert os.path.exists(config['run_settings']['cluster_config']), print(f"\nMust provide a cluster configuration file under run_settings -> cluster_config in the config.yml file\nFile \'{config['run_settings']['cluster_config']}\' does not exist\n")
    clusterfile = yaml.load(open(config['run_settings']['cluster_config']), Loader=yaml.FullLoader)
    for rule in clusterfile.keys(): # Purpose of this code is to make sure plink requests same amount of memory/threads as specified in cluster.yaml file
        if config['run_settings']['scheduler'] == 'slurm': plink_run_specs[rule] = f"--memory {int(int(clusterfile[rule]['mem'].replace('G','000'))*0.9)} --threads {clusterfile[rule]['ntasks']}"
        elif config['run_settings']['scheduler'] == 'pbs':  plink_run_specs[rule] = f"--memory {clusterfile[rule]['mem'].replace('b','').replace('m','').replace('g','000')} --threads 1"


# Conditionally set expected outputs of various steps depending on flags in config.yml
def get_all_inputs(wildcards):
    input_list = [f"{OUT}-report.pdf"] #, f"{OUT}-rulegraph.png"
    if config['merge'] == 'true':
        input_list += [f"{OUT}.bed"]
    input_list += expand(f"{{rawdata}}/{{rawdata}}.chr{{chrom}}.vcf.gz", rawdata=QUERY.keys(), chrom=CHROMS)
    if config['reference']['download'] == 'true':
        input_list += [REFASTA, REFVCF]
    return(input_list)

def get_FixRef_input(wildcards):
    input = [f"{wildcards.rawdata}/tmp/{wildcards.rawdata}_flips.bed"]
    if config['FixRef'].lower() == "true": input += [REFASTA,REFVCF]
    return(input)

def plnk_rsrc(rule_name, plink_specs):
    try: resources = plink_specs[rule_name] # Grab resource specifications from dict created earlier based on rule_name
    except KeyError: resources = plink_specs['__default__'] # Set to default specifications if resources for rule_name not specified
    return(resources)

rule all: #Main function that determines which files snakemake will attempt to produce upon running
    input: get_all_inputs

rule clean: # run snakemake clean to clear all (or most) of the created output
    params: dir_list = " ".join([f"{os.getcwd()}/{x}/*" for x in QUERY.keys()])
    run:
        for key in QUERY.keys(): #Loop over and remove subdirectories that were created for each input dataset
            shutil.rmtree(key)
        if os.path.exists("RefAnnotationData"): shutil.rmtree("RefAnnotationData")
        if os.path.exists(f"{OUT}-report.pdf"): os.remove(f"{OUT}-report.pdf")
        if os.path.exists(f"{OUT}.bed"): os.remove(f"{OUT}.bed")

# rule make_rulegraph:
#     output: f"{OUT}-rulegraph.png"
#     shell: f"set +u; snakemake --rulegraph --nolock --configfile workflow/config.yml > accessory/Pipeline_DAG.dot; {CMD_PREFIX} dot -Tpng {os.getcwd()}/accessory/Pipeline_DAG.dot > {{output}}"

rule update_alleles_IDs: # (Optionally,) Update chromosome labels, SNP IDs, and alleles
    output: "{rawdata}/tmp/{rawdata}_alleles_IDs.bed"
    params:
        in_pre = lambda wildcards: QUERY[wildcards.rawdata]['data'],
        prefix = "{rawdata}/tmp/{rawdata}",
        exec = f"{CMD_PREFIX} plink",
        alleles = lambda wildcards: QUERY[wildcards.rawdata]['allele_key'],
        ids = lambda wildcards: QUERY[wildcards.rawdata]['ID_key'],
        chrom_ids = lambda wildcards: QUERY[wildcards.rawdata]['chrom_key']
    run:
        shell(f"cut -f 2 {{params.in_pre}}.bim | sort | uniq -d > {{params.prefix}}.dups")#; {CMD_PREFIX} sed -i 's/_/-/g' {{params.in_pre}}.fam") #having this is in command is bad practice.  Shouldn't modify original files
        if params.ids != "none":
            if params.alleles != "none":
                if params.chrom_ids != "none":
                    shell(f"{{params.exec}} --bfile {{params.in_pre}} --snps-only --keep-allele-order --exclude {{params.prefix}}.dups --update-chr {{params.chrom_ids}} --make-bed --out {{params.prefix}}.chrom {plnk_rsrc(rule, plink_run_specs)}; "
                          f"{{params.exec}} --bfile {{params.prefix}}.chrom --keep-allele-order --update-alleles {{params.alleles}} --make-bed --out {{params.prefix}}_alleles {plnk_rsrc(rule, plink_run_specs)}; "
                          f"{{params.exec}} --bfile {{params.prefix}}_alleles --keep-allele-order --update-name {{params.ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
                else:
                    shell(f"{{params.exec}} --bfile {{params.in_pre}} --snps-only --keep-allele-order --exclude {{params.prefix}}.dups --update-alleles {{params.alleles}} --make-bed --out {{params.prefix}}_alleles {plnk_rsrc(rule, plink_run_specs)}; "
                          f"{{params.exec}} --bfile {{params.prefix}}_alleles --keep-allele-order --update-name {{params.ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
            elif params.chrom_ids != "none":
                shell(f"{{params.exec}} --bfile {{params.in_pre}} --snps-only --keep-allele-order --exclude {{params.prefix}}.dups --update-chr {{params.chrom_ids}} --make-bed --out {{params.prefix}}.chrom {plnk_rsrc(rule, plink_run_specs)}; "
                      f"{{params.exec}} --bfile {{params.prefix}}.chrom --keep-allele-order --update-name {{params.ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
            else:
                shell(f"{{params.exec}} --bfile {{params.in_pre}} --snps-only --keep-allele-order --exclude {{params.prefix}}.dups --update-name {{params.ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
        else:
            if params.alleles != "none":
                if params.chrom_ids != "none":
                    shell(f"{{params.exec}} --bfile {{params.in_pre}} --snps-only --keep-allele-order --exclude {{params.prefix}}.dups --update-chr {{params.chrom_ids}} --make-bed --out {{params.prefix}}.chrom {plnk_rsrc(rule, plink_run_specs)}; "
                          f"{{params.exec}} --bfile {{params.prefix}}.chrom --keep-allele-order --update-alleles {{params.alleles}} --make-bed --out {{params.prefix}}_alleles")
                else:
                    shell(f"{{params.exec}} --bfile {{params.in_pre}} --snps-only --keep-allele-order --exclude {{params.prefix}}.dups --update-alleles {{params.alleles}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
            elif params.chrom_ids != "none":
                shell(f"{{params.exec}} --bfile {{params.in_pre}} --snps-only --keep-allele-order --exclude {{params.prefix}}.dups --update-chr {{params.chrom_ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
            else:
                shell(f"{{params.exec}} --snps-only --keep-allele-order --bfile {{params.in_pre}} --exclude {{params.prefix}}.dups --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")

rule flip_strands: #Flip strand for the SNPs specified in 'flip_key' in config file
    input: "{rawdata}/tmp/{rawdata}_alleles_IDs.bed"
    output: "{rawdata}/tmp/{rawdata}_flips.bed"
    params: flip_key = lambda wildcards: QUERY[wildcards.rawdata]['flip_key']
    run:
        shell(f"cut -f 2 {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_alleles_IDs.bim | sort | uniq -d > {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups;"
              f"{CMD_PREFIX} plink --bfile {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_alleles_IDs --exclude {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups --keep-allele-order --make-bed --out {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups {plnk_rsrc(rule, plink_run_specs)}")
        if params.flip_key != "none":
            shell(f"{CMD_PREFIX} plink --bfile {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups --keep-allele-order --flip {{params.flip_key}} --make-bed --out {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_flips {plnk_rsrc(rule, plink_run_specs)}")
        else:
            shell(f"{CMD_PREFIX} plink --bfile {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups --keep-allele-order --make-bed --out {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_flips {plnk_rsrc(rule, plink_run_specs)}")

rule download_Refs:  # Download reference fasta and VCF file
    output: REFVCF, REFASTA
    shell: "{CMD_PREFIX} wget --directory-prefix=./RefAnnotationData/ {REFASTA};"
           "{CMD_PREFIX} wget --directory-prefix=./RefAnnotationData/ {REFVCF}"

rule FixRef:  # Use bcftools +fixref to set reference vs alternate alleles
    input: get_FixRef_input #"{rawdata}/tmp/{rawdata}_flips.bed", REFASTA, REFVCF
    output: "{rawdata}/tmp/{rawdata}-FixRef.bed"
    params:
        prefix = "{rawdata}/tmp/{rawdata}",
        chrom_key = lambda wildcards: QUERY[wildcards.rawdata]['chrom_key']
    run:
        if config['FixRef'] == "true":
            if not input[1].endswith(".gz"): #Input VCFs need to be compressed with bgzip
                shell(f"{CMD_PREFIX} bgzip -c {{input[1]}} > {wildcards.rawdata}/tmp/tmp.fasta.gz")
            else:
                shell(f"{CMD_PREFIX} sh -c 'zcat {{input[1]}} | bgzip -c > {wildcards.rawdata}/tmp/tmp.fasta.gz'")
            shell(f"{CMD_PREFIX} python -c 'import pysam; pysam.faidx(\"{wildcards.rawdata}/tmp/tmp.fasta.gz\")'") #Index FASTA file
            if not os.path.exists(params.chrom_key):
                params.chrom_key = f"{os.getcwd()}/accessory/PlinkChrRename.txt" #This ought to be downloaded along with repo, so can use this by default if a chromosome key is not provided
            shell(f"{CMD_PREFIX} bash {CODE}/FixRef.sh {{wildcards.rawdata}}_flips {{wildcards.rawdata}}/tmp/ {{wildcards.rawdata}}-FixRef {params.chrom_key} {os.getcwd()}/{wildcards.rawdata}/tmp/tmp.fasta.gz {{input[2]}} t t t t")
        else:
            shell(f"{CMD_PREFIX} plink --bfile {{params.prefix}}_flips --keep-allele-order --make-bed --out {{params.prefix}}-FixRef {plnk_rsrc(rule, plink_run_specs)}")

rule set_pheno: #Set phenotypes
    input: "{rawdata}/tmp/{rawdata}-FixRef.bed"
    output: "{rawdata}/{rawdata}-FixRef-pheno.bed"
    run:
        if os.path.exists(config['query'][wildcards.rawdata]['pheno_file']):
            shell(f"{CMD_PREFIX} plink --bfile {wildcards.rawdata}/tmp/{wildcards.rawdata}-FixRef --pheno {config['query'][wildcards.rawdata]['pheno_file']} --keep-allele-order --make-bed --out {wildcards.rawdata}/{wildcards.rawdata}-FixRef-pheno")
        else:
            shell(f"{CMD_PREFIX} plink --bfile {wildcards.rawdata}/tmp/{wildcards.rawdata}-FixRef --pheno {config['query'][wildcards.rawdata]['data']}.bim --mpheno 4 --keep-allele-order --make-bed --out {wildcards.rawdata}/{wildcards.rawdata}-FixRef-pheno")

rule set_sex: #Set sex
    input: "{rawdata}/{rawdata}-FixRef-pheno.bed"
    output: "{rawdata}/{rawdata}-FixRef-pheno-sex.bed"
    run:
        if os.path.exists(config['query'][wildcards.rawdata]['sex_file']):
            shell(f"{CMD_PREFIX} plink --bfile {wildcards.rawdata}/{wildcards.rawdata}-FixRef-pheno --update-sex {config['query'][wildcards.rawdata]['sex_file']} --keep-allele-order --make-bed --out {wildcards.rawdata}/{wildcards.rawdata}-FixRef-pheno-sex")
        else:
            shell(f"{CMD_PREFIX} plink --bfile {wildcards.rawdata}/{wildcards.rawdata}-FixRef-pheno --pheno {config['query'][wildcards.rawdata]['data']}.bim --mpheno 3 --keep-allele-order --make-bed --out {wildcards.rawdata}/{wildcards.rawdata}-FixRef-pheno-sex")

rule QCcombine_query: #Run various QC steps
    input: "{rawdata}/{rawdata}-FixRef-pheno-sex.bed"
    output: "{rawdata}/{rawdata}-QC.bed"
    params:
        pre1 = "{rawdata}/tmp/{rawdata}",
        pre2 = "{rawdata}/{rawdata}",
        tvm1 = config['QC']['vm1'], tgm = config['QC']['gm'], tvm2 = config['QC']['vm2'],
        hwe = config['QC']['hwe'], maf = config['QC']['maf'], mbs = config['QC']['mbs'], mbc = config['QC']['mbc'],
    run:
        if config['perform_QC'] == 'true':
            with open("scripts/QCcombine_query.sh", 'w') as QCsh:
                cmd = f"{CMD_PREFIX} {PYTHON} {CODE}/QC.py -i {{params.pre1}}-FixRef -d {{wildcards.rawdata}}/tmp/ -o {{wildcards.rawdata}}-QC " \
                      f"-p plink -tvm1 {{params.tvm1}} -tgm {{params.tgm}} -tvm2 {{params.tvm2}} -hwe {{params.hwe}} -mbc {{params.mbc}} -mbs {{params.mbs}} -maf {{params.maf}} -r '{plnk_rsrc(rule, plink_run_specs)}'; " \
                      f"mv {{params.pre1}}-QC.bed {{wildcards.rawdata}}; mv {{params.pre1}}-QC.bim {{wildcards.rawdata}}; " \
                      f"mv {{params.pre1}}-QC.fam {{wildcards.rawdata}};"
                if config['delete_intermediates'] == 'true': cmd += f"rm -r {{wildcards.rawdata}}/tmp/"
            shell(cmd)
        else:
            shell(f"{CMD_PREFIX} plink --bfile {{params.pre1}}-FixRef --make-bed --out {{params.pre2}}-QC {plnk_rsrc(rule, plink_run_specs)}")
        if config['FixRef'].lower() == "true": shell(f"cp {{wildcards.rawdata}}/tmp/TEMP/DataFixStep3_{{wildcards.rawdata}}_flips-RefFixSorted.bim accessory/")

rule restore_strand: #This step ensures that the A2 allele specification is maintained from previous steps
    input: "{rawdata}/{rawdata}-QC.bed"
    output: "{rawdata}/{rawdata}-QC-Ref.bed"
    params: pre2 = "{rawdata}/{rawdata}"
    run:
        if config['FixRef'].lower() == "true": bim_file = f"accessory/DataFixStep3_{{wildcards.rawdata}}_flips-RefFixSorted.bim"
        else: bim_file = f"{{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_flips.bim"
        shell(f"{CMD_PREFIX} plink --bfile {{params.pre2}}-QC --make-bed --a2-allele {bim_file} 6 2 --out {{params.pre2}}-QC-Ref {plnk_rsrc(rule, plink_run_specs)}")

rule merge_inputs: #Optionally, merge together the different datasets provided.
    input: expand("{rawdata}/{rawdata}-QC-Ref.bed", rawdata=QUERY.keys())
    output: f"{OUT}.bed"
    params:
        input_list = ",".join([f"{x}/{x}-QC-Ref" for x in QUERY.keys()]) #Create comma separated list of plink prefixes
    run:
        if config['merge'] == 'true':
            shell(f"{CMD_PREFIX} {PYTHON} {CODE}/mergeInputs.py -i {{params.input_list}} -d {os.getcwd()} -o {OUT} -p plink")

rule mis_impute_prep: #Parse PLINK datasets by chromosome and convert to VCF for imputation on TOPMed
    input: "{rawdata}/{rawdata}-QC-Ref.bed"
    output: "{rawdata}/{rawdata}.chr{chrom}.vcf.gz"
    params:
        pre = "{rawdata}/{rawdata}"
    run:
        if config["build"] == "38":
            shell(f" module load bcftools; "
                  f" {CMD_PREFIX} plink --bfile {{params.pre}}-QC-Ref --chr {{wildcards.chrom}} --recode vcf-iid --keep-allele-order --out {{params.pre}}.chr{{wildcards.chrom}} {plnk_rsrc(rule, plink_run_specs)}; "
                  f" {CMD_PREFIX} bcftools sort {{params.pre}}.chr{{wildcards.chrom}}.vcf | bcftools annotate --rename-chrs accessory/bcftoolsChrRename.txt -Oz -o {{params.pre}}.chr{{wildcards.chrom}}.vcf.gz; "
                  f" rm {{params.pre}}.chr{{wildcards.chrom}}.vcf")
        else:
            shell(f" module load bcftools; "
                  f" {CMD_PREFIX} plink --bfile {{params.pre}}-QC-Ref --chr {{wildcards.chrom}} --recode vcf-iid --keep-allele-order --out {{params.pre}}.chr{{wildcards.chrom}} {plnk_rsrc(rule, plink_run_specs)}; "
                  f" {CMD_PREFIX} bcftools sort -Oz -o {{params.pre}}.chr{{wildcards.chrom}}.vcf.gz {{params.pre}}.chr{{wildcards.chrom}}.vcf; "
                  f" rm {{params.pre}}.chr{{wildcards.chrom}}.vcf")

#TODO: add mbc filter to QC report
rule make_report: #Final step to create a PDF report that summarizes the most relevant results.
    input: expand("{rawdata}/{rawdata}.chr{chrom}.vcf.gz", rawdata=QUERY.keys(), chrom=CHROMS)#, f"{OUT}-rulegraph.png"
    output: f"{OUT}-report.pdf"
    params: input_list = ",".join(expand("{rawdata}/{rawdata}.chr20_freq.txt", rawdata=QUERY.keys()))
    run:
        with open("scripts/gather_report_data.sh", 'w') as report_cmds: #This shell script generates some ancillary data and also formats a call to render the R(markdown) with the specified arguments.
            report_cmds.write("wc -l " + os.getcwd() + "/*/tmp/*txt | grep -v -e scripts -e total | awk \'{A=split($2,a,\"/\"); split(a[A],b,\".\"); split(b[1],c,\"-\"); print c[1],b[3],$1}\' > " + os.getcwd() + "/accessory/filter.stats\n") #Determine number of SNPs caught by each of the QC steps for each of the datasets
            report_cmds.write("wc -l " + os.getcwd() + "/*/tmp/*flips.bim | grep -v -e total | awk \'{A=split($2,a,\"/\"); split(a[A],b,\".\"); split(b[1],c,\"_\"); printf(\"%s raw %s\\n\", c[1],$1)}\' >> " + os.getcwd() + "/accessory/filter.stats\n") #Determie raw number of SNPs prior to QC
            report_cmds.write("for f in " + os.getcwd() + "/OandE/FixRef.rawdata*err; do awk \'{print FILENAME, $0}\' $f | tail -q -n 40 | grep -e NS | grep -v Number | cut -d \"=\" -f 2 | awk \'{split($1,a,\".\"); print a[1],$3,$4,$5}\' | sed \'s/ref /ref-/\' | sed \'s/fixed /fixed-/\' | sed \'s/%//\'; done > " + os.getcwd() + "/accessory/FixRef.stats\n") #Extract stats from the FixRef step
            report_cmds.write("tabix -f " + REFVCF + "; tabix " + REFVCF +  " 20 | vawk \'{split(I$CAF,a,\",\"); if($1==20 && a[2] > 0) print $1,$2,$3,a[2]; split(I$AF,a,\",\"); if($1==20 && a[1] > 0) print $1,$2,$3,a[1]}\' | awk \'{if(NF==4) print $0}\' > " + os.getcwd() + "/accessory/ref_freqs.txt\n") #Calculate allele frequencies in the reference VCF to be used as verification that SNPs have been flipped to correct strand in query datasets
            report_cmds.write("for f in " + os.getcwd() + "/*/*chr20.vcf.gz; do zgrep -v \"#\" $f | shuf -n 10000 |  vawk \'{print $1,$2,$3, S$*$GT}\' | tr \"/\" \" \" | tr \":\" \" \" > ${f%.vcf.gz}_freq.txt; done\n") #Get allele frequencies from each dataset for comparison with reference frequencies.
            #Format call to RMarkdown for report creation.
            report_line = f"echo \'rmarkdown::render(\"scripts/DataPrep_report.Rmd\", output_file=\"{OUT}-report.pdf\", " \
                          f"params=list(flt_file=\"accessory/filter.stats\", " \
                          f"FixRef_file=\"accessory/FixRef.stats\"," \
                          f"rulegraph_file=\"Pipeline_DAG.png\", " \
                          f"ref_freqs=\"accessory/ref_freqs.txt\", " \
                          f"dat_freq=\"{params.input_list}\", " \
                          f"config_file=\"workflow/config.yml\"))\' | R --vanilla"
            report_cmds.write(report_line)
        cmd = f"{CMD_PREFIX} sh scripts/gather_report_data.sh; mv scripts/{OUT}-report.pdf {OUT}-report.pdf"
        shell(cmd)

