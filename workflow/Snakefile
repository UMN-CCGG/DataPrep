# Load modules
import glob
import os
import subprocess
import pdb
import shutil
import yaml

# Get the date
from datetime import datetime
i = datetime.now()
TIME = i.strftime('%Y-%m-%d')

# Specify config file
configfile: "workflow/config.yml"



#TODO: Add reading of cluster.yaml file to better handle memory and thread specs in PLINK

# Parse config.yml file
OUT = config['outname']
QUERY = config['query']
BCFTOOLS = config['bcftools']['executable']
PYTHON = config['python']

# Is reference info provided in config file or do we download?
REFASTA = config['reference']['refFasta']
REFVCF = config['reference']['refVCF']
if config['FixRef'] == "true":
    if config['download_reference'] == 'true':
        REFASTA = f"{os.getcwd()}/RefAnnotationData/human_g1k_v37.fasta.gz"
        REFVCF = f"{os.getcwd()}/RefAnnotationData/All_20170710.vcf.gz"
    else:
        assert (REFASTA != "none" and REFVCF != "none"), "Must provide a reference fasta and VCF in workflow/config.yml file"
        assert os.path.exists(config['reference']['refFasta']), f"Did not find reference fasta: {config['reference']['refFasta']}"
        assert os.path.exists(config['reference']['refVCF']), f"Did not find reference VCF: {config['reference']['refVCF']}"

# Format commands and bind paths if using singularity.
if config['singularity']['use_singularity'] == 'true' and config['singularity']['image'] != "none":
    query_paths = []
    for x in QUERY.keys():
        for k,v in QUERY[x].items():
            path = os.path.dirname(str(v))
            if path not in query_paths: query_paths.append(path)
    bind_paths = ",".join(set(query_paths + [os.path.dirname(REFASTA), os.path.dirname(REFVCF), 'OandE', 'accessory', config['singularity']['code']] + list(QUERY.keys())))
    CMD_PREFIX = f"set +u; {config['singularity']['module']}; singularity exec --bind {bind_paths},{os.getcwd()} {config['singularity']['image']}"
    CODE = config['singularity']['code']
else:
    CMD_PREFIX = config['cmd_prefix']
    CODE = config['dir']['code']


# Make subdirectories
if not os.path.exists("RefAnnotationData"): os.mkdir("RefAnnotationData")
if not os.path.exists("accessory"): os.mkdir("accessory")
if not os.path.exists("OandE"): os.mkdir("OandE")
for datset in QUERY.keys():
    if not os.path.exists(datset):
        os.mkdir(datset)
        os.mkdir(f"{datset}/tmp")

CHROMS = config['chroms']
if CHROMS == 'all':
    CHROMS = [str(x) for x in range(1, 23)]
    if config['include_x'] == 'yes': CHROMS.append('X')

# Run locally or submit to cluster depending on config.yml
plink_run_specs = {'__default__': ""} # Set default to empty string in case of local run.  i.e. default to plink's auto-detection of resources.
if config['run_settings']['local_run'] == 'true':
    localrules: all, update_alleles_IDs, FixRef, QCcombine_query, set_pheno
else:
    localrules: all, make_rulegraph, set_pheno
    assert config['run_settings']['scheduler'] in ['pbs', 'slurm'], print(f"\nThe scheduler specified at run_settings -> scheduler: \'{config['run_settings']['scheduler']}\' is invalid.\n")
    assert os.path.exists(config['run_settings']['cluster_config']), print(f"\nMust provide a cluster configuration file under run_settings -> cluster_config in the config.yml file\nFile \'{config['run_settings']['cluster_config']}\' does not exist\n")
    clusterfile = yaml.load(open(config['run_settings']['cluster_config']), Loader=yaml.FullLoader)
    for rule in clusterfile.keys(): # Purpose of this code is to make sure plink requests same amount of memory/threads as specified in cluster.yaml file
        if config['run_settings']['scheduler'] == 'slurm': plink_run_specs[rule] = f"--memory {int(clusterfile[rule]['mem-per-cpu'].replace('G','000')) * int(clusterfile[rule]['ntasks'])} --threads {clusterfile[rule]['ntasks']}"
        elif config['run_settings']['scheduler'] == 'pbs':  plink_run_specs[rule] = f"--memory {clusterfile[rule]['mem'].replace('b','').replace('m','').replace('g','000')} --threads 1"


# Conditionally set expected outputs of various steps depending on flags in config.yml
def get_all_inputs(wildcards):
    input_list = [f"{OUT}-report.pdf", f"{OUT}-rulegraph.png"]
    if config['merge'] == 'true':
        input_list += [f"{OUT}.bed"]
    if config['impute_prep'] == 'true':
        input_list += expand(f"{{rawdata}}/{{rawdata}}.chr{{chrom}}.vcf.gz", rawdata=QUERY.keys(), chrom=CHROMS)
    if config['download_reference'] == 'true':
        input_list += [REFASTA, REFVCF]
    return(input_list)

def plnk_rsrc(rule_name, plink_specs):
    try: resources = plink_specs[rule_name] # Grab resource specifications from dict created earlier based on rule_name
    except KeyError: resources = plink_specs['__default__'] # Set to default specifications if resources for rule_name not specified
    return(resources)

rule all:
    input: get_all_inputs
    # input: expand(f"{{rawdata}}/{{rawdata}}.chr{{chrom}}.vcf.gz", rawdata=QUERY.keys(), chrom=CHROMS), f"{OUT}-report.pdf", f"{OUT}-rulegraph.png"

rule clean: # run snakemake clean to clear all (or most) of the created output
    params: dir_list = " ".join([f"{os.getcwd()}/{x}/*" for x in QUERY.keys()])
    run:
        for key in QUERY.keys():
            shutil.rmtree(key)
        if os.path.exists("RefAnnotationData"): shutil.rmtree("RefAnnotationData")
        if os.path.exists(f"{OUT}-report.pdf"): os.remove(f"{OUT}-report.pdf")
        if os.path.exists(f"{OUT}.bed"): os.remove(f"{OUT}.bed")

rule make_rulegraph:
    output: f"{OUT}-rulegraph.png"
    params: mod_cmd = CMD_PREFIX.replace(f"set +u; {config['singularity']['module']};", "")
    shell: f"{config['singularity']['module']}; snakemake --rulegraph --configfile workflow/config.yml | {{params.mod_cmd}} dot -Tpng > {{output}}"

rule update_alleles_IDs: # Update chromosome labels, SNP IDs, and alleles
    output: "{rawdata}/tmp/{rawdata}_alleles_IDs.bed"
    params:
        in_pre = lambda wildcards: QUERY[wildcards.rawdata]['data'],
        prefix = "{rawdata}/tmp/{rawdata}",
        exec = f"{CMD_PREFIX} plink",
        alleles = lambda wildcards: QUERY[wildcards.rawdata]['allele_key'],
        ids = lambda wildcards: QUERY[wildcards.rawdata]['ID_key'],
        chrom_ids = lambda wildcards: QUERY[wildcards.rawdata]['chrom_key']
    run:
        shell(f"cut -f 2 {{params.in_pre}}.bim | sort | uniq -d > {{params.prefix}}.dups; {CMD_PREFIX} sed -i 's/_/-/g' {{params.in_pre}}.fam")
        if params.ids != "none":
            if params.alleles != "none":
                if params.chrom_ids != "none":
                    shell(f"{{params.exec}} --bfile {{params.in_pre}} --keep-allele-order --exclude {{params.prefix}}.dups --update-chr {{params.chrom_ids}} --make-bed --out {{params.prefix}}.chrom {plnk_rsrc(rule, plink_run_specs)}; "
                          f"{{params.exec}} --bfile {{params.prefix}}.chrom --keep-allele-order --update-alleles {{params.alleles}} --make-bed --out {{params.prefix}}_alleles {plnk_rsrc(rule, plink_run_specs)}; "
                          f"{{params.exec}} --bfile {{params.prefix}}_alleles --keep-allele-order --update-name {{params.ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
                else:
                    shell(f"{{params.exec}} --bfile {{params.in_pre}} --keep-allele-order --exclude {{params.prefix}}.dups --update-alleles {{params.alleles}} --make-bed --out {{params.prefix}}_alleles {plnk_rsrc(rule, plink_run_specs)}; "
                          f"{{params.exec}} --bfile {{params.prefix}}_alleles --keep-allele-order --update-name {{params.ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
            elif params.chrom_ids != "none":
                shell(f"{{params.exec}} --bfile {{params.in_pre}} --keep-allele-order --exclude {{params.prefix}}.dups --update-chr {{params.chrom_ids}} --make-bed --out {{params.prefix}}.chrom {plnk_rsrc(rule, plink_run_specs)}; "
                      f"{{params.exec}} --bfile {{params.prefix}}.chrom --keep-allele-order --update-name {{params.ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
            else:
                shell(f"{{params.exec}} --bfile {{params.in_pre}} --keep-allele-order --exclude {{params.prefix}}.dups --update-name {{params.ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
        else:
            if params.alleles != "none":
                if params.chrom_ids != "none":
                    shell(f"{{params.exec}} --bfile {{params.in_pre}} --keep-allele-order --exclude {{params.prefix}}.dups --update-chr {{params.chrom_ids}} --make-bed --out {{params.prefix}}.chrom {plnk_rsrc(rule, plink_run_specs)}; "
                          f"{{params.exec}} --bfile {{params.prefix}}.chrom --keep-allele-order --update-alleles {{params.alleles}} --make-bed --out {{params.prefix}}_alleles")
                else:
                    shell(f"{{params.exec}} --bfile {{params.in_pre}} --keep-allele-order --exclude {{params.prefix}}.dups --update-alleles {{params.alleles}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
            elif params.chrom_ids != "none":
                shell(f"{{params.exec}} --bfile {{params.in_pre}} --keep-allele-order --exclude {{params.prefix}}.dups --update-chr {{params.chrom_ids}} --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")
            else:
                shell(f"{{params.exec}} --keep-allele-order --bfile {{params.in_pre}} --exclude {{params.prefix}}.dups --make-bed --out {{params.prefix}}_alleles_IDs {plnk_rsrc(rule, plink_run_specs)}")

rule flip_strands:
    input: "{rawdata}/tmp/{rawdata}_alleles_IDs.bed"
    output: "{rawdata}/tmp/{rawdata}_flips.bed"
    params: flip_key = lambda wildcards: QUERY[wildcards.rawdata]['flip_key']
    run:
        shell(f"cut -f 2 {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_alleles_IDs.bim | sort | uniq -d > {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups;"
              f"{CMD_PREFIX} plink --bfile {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_alleles_IDs --exclude {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups --keep-allele-order --make-bed --out {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups {plnk_rsrc(rule, plink_run_specs)}")
        if params.flip_key != "none":
            shell(f"{CMD_PREFIX} plink --bfile {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups --keep-allele-order --flip {{params.flip_key}} --make-bed --out {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_flips {plnk_rsrc(rule, plink_run_specs)}")
        else:
            shell(f"{CMD_PREFIX} plink --bfile {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}.2.dups --keep-allele-order --make-bed --out {{wildcards.rawdata}}/tmp/{{wildcards.rawdata}}_flips {plnk_rsrc(rule, plink_run_specs)}")

rule download_Refs:  # Download hg19 and the 1000Genomes VCF for use in strand flipping of query data
    output: REFVCF, REFASTA
    shell: "{CMD_PREFIX} sh {CODE}/DownloadRefs.sh"

rule FixRef:  # Use bcftools +fixref to set reference vs alternate alleles
    input: "{rawdata}/tmp/{rawdata}_flips.bed", REFASTA, REFVCF
    output: "{rawdata}/tmp/{rawdata}-FixRef.bed"
    params:
        prefix = "{rawdata}/tmp/{rawdata}",
        chrom_key = lambda wildcards: QUERY[wildcards.rawdata]['chrom_key']
    run:
        if config['FixRef'] == "true":
            if not input[1].endswith(".gz"):
                shell(f"{CMD_PREFIX} gzip {{input[1]}}")
                input[1] += ".gz"
            shell(f"{CMD_PREFIX} bash {CODE}/FixRef.sh {{wildcards.rawdata}}_flips {{wildcards.rawdata}}/tmp/ {{wildcards.rawdata}}-FixRef {params.chrom_key} {{input[1]}} {{input[2]}} t t t t")
        else:
            shell(f"{CMD_PREFIX} plink --bfile {{params.prefix}}_flips --keep-allele-order --make-bed --out {{params.prefix}}-FixRef {plnk_rsrc(rule, plink_run_specs)}")

rule QCcombine_query:
    input: "{rawdata}/tmp/{rawdata}-FixRef.bed"
    output: "{rawdata}/{rawdata}-QC.bed"
    params:
        pre1 = "{rawdata}/tmp/{rawdata}",
        pre2 = "{rawdata}/{rawdata}",
        tvm1 = config['QC']['vm1'], tgm = config['QC']['gm'], tvm2 = config['QC']['vm2'],
        hwe = config['QC']['hwe'], maf = config['QC']['maf'], mbs = config['QC']['mbs'],
    run:
        if config['perform_QC'] == 'true':
            with open("scripts/QCcombine_query.sh", 'w') as QCsh:
                cmd = f"{CMD_PREFIX} {PYTHON} {CODE}/QC.py -i {{params.pre1}}-FixRef -d {{wildcards.rawdata}}/tmp/ -o {{wildcards.rawdata}}-QC " \
                      f"-p plink -tvm1 {{params.tvm1}} -tgm {{params.tgm}} -tvm2 {{params.tvm2}} -hwe {{params.hwe}} -mbs {{params.mbs}} -maf {{params.maf}} -r '{plnk_rsrc(rule, plink_run_specs)}'; " \
                      f"mv {{params.pre1}}-QC.bed {{wildcards.rawdata}}; mv {{params.pre1}}-QC.bim {{wildcards.rawdata}}; " \
                      f"mv {{params.pre1}}-QC.fam {{wildcards.rawdata}}; cp {{wildcards.rawdata}}/tmp/TEMP/DataFixStep3_{{wildcards.rawdata}}_flips-RefFixSorted.bim accessory/"
                if config['delete_intermediates'] == 'true': cmd += f"rm -r {{wildcards.rawdata}}/tmp/"
            shell(cmd)
        else:
            shell(f"{CMD_PREFIX} plink --bfile {{params.pre1}}-FixRef --make-bed --out {{params.pre2}}-QC {plnk_rsrc(rule, plink_run_specs)}")

#Only execute if FixRef is executed.  Can't delete intermediates from prior step cuz they might be needed.  What about copying bim from TEMP and put it in accessory prior to deleting.
rule restore_strand:
    input: "{rawdata}/{rawdata}-QC.bed"
    output: "{rawdata}/{rawdata}-QC-Ref.bed"
    params:
        bim_file = "accessory/DataFixStep3_{rawdata}_flips-RefFixSorted.bim",
        pre1 = "{rawdata}/tmp/{rawdata}",
        pre2 = "{rawdata}/{rawdata}"
    shell: f"{CMD_PREFIX} plink --bfile {{params.pre2}}-QC --make-bed --a2-allele {{params.bim_file}} 6 2 --out {{params.pre2}}-QC-Ref {plnk_rsrc(rule, plink_run_specs)}"

rule set_pheno:
    input: "{rawdata}/{rawdata}-QC-Ref.bed"
    output: "{rawdata}/.{rawdata}-PhenoFixed"
    run:
        if f"{wildcards.rawdata}" in config['case_datasets'].strip().split(","):
            shell("{CMD_PREFIX} sed -i 's/-9$/2/' {wildcards.rawdata}/{wildcards.rawdata}-QC-Ref.fam; touch {output}")
        elif f"{wildcards.rawdata}" in config['control_datasets'].strip().split(","):
            shell("{CMD_PREFIX} sed -i 's/-9$/1/' {wildcards.rawdata}/{wildcards.rawdata}-QC-Ref.fam; touch {output}")
        else:
            shell("touch {output}")

rule merge_inputs:
    input: expand("{rawdata}/{rawdata}-QC-Ref.bed", rawdata=QUERY.keys())
    output: f"{OUT}.bed"
    params:
        input_list = ",".join([f"{x}/{x}-QC-Ref" for x in QUERY.keys()])
    run:
        if config['merge'] == 'true':
            shell(f"{CMD_PREFIX} {PYTHON} {CODE}/mergeInputs.py -i {{params.input_list}} -d {os.getcwd()} -o {OUT} -p plink")

# TODO: Flesh out phenotype labelling in order to implement the (optional?) filter for missingness by case/control
# ## CASE/CONTROLS ARE NOT YET LABELLED AT THIS POINT
# rule filter_merged:
#     input: f"{OUT}.bed"
#     output: f"{OUT}_flt.bed"
#     run:
#         import pandas as pd
#         shell(f"{CMD_PREFIX} plink --bfile {OUT} --test-missing --out {OUT}")
#         dat = pd.read_table(f"{OUT}.missing", header=None)
#         dat2 = dat[dat['P'] < int(config['QC']['mbc'])]
#         dat2['SNP'].to_csv('accessory/mbc_filter.txt', header=False)
#         shell("f{CMD_PREFIX} plink --bfile {OUT} --exclude accessory/mbc_filter.txt --keep-allele-order --make-bed --out {OUT}_flt")


rule mis_impute_prep:
    input: "{rawdata}/{rawdata}-QC-Ref.bed", "{rawdata}/.{rawdata}-PhenoFixed"
    output: "{rawdata}/{rawdata}.chr{chrom}.vcf.gz"
    params:
        pre = "{rawdata}/{rawdata}",
    run:
        shell(f"{CMD_PREFIX} plink --bfile {{params.pre}}-QC-Ref --chr {{wildcards.chrom}} --recode vcf-iid --keep-allele-order --out {{params.pre}}.chr{{wildcards.chrom}} {plnk_rsrc(rule, plink_run_specs)}; "
              f" {CMD_PREFIX} bcftools sort -Oz -o {{params.pre}}.chr{{wildcards.chrom}}.vcf.gz {{params.pre}}.chr{{wildcards.chrom}}.vcf; rm {{params.pre}}.chr{{wildcards.chrom}}.vcf")


#TODO: grepping in line 204 is not specific to latest run.  Numbers from older runs are included as well.
#TODO: add mbc filter to QC report
rule make_report:
    input: expand("{rawdata}/{rawdata}.chr{chrom}.vcf.gz", rawdata=QUERY.keys(), chrom=CHROMS), f"{OUT}-rulegraph.png"
    output: f"{OUT}-report.pdf"
    params: input_list = ",".join(expand("{rawdata}/{rawdata}.chr20_freq.txt", rawdata=QUERY.keys()))
    run:
        with open("scripts/gather_report_data.sh", 'w') as report_cmds:
            report_cmds.write("wc -l */tmp/*txt | grep -v -e scripts -e total | awk \'{split($2,a,\"/\"); split(a[3],b,\".\"); split(b[1],c,\"-\"); print c[1],b[3],$1}\' > accessory/filter.stats\n")
            report_cmds.write("wc -l */tmp/*flips.bim | grep -v -e total | awk \'{split($2,a,\"/\"); split(a[3],b,\".\"); split(b[1],c,\"_\"); printf(\"%s raw %s\\n\", c[1],$1)}\' >> accessory/filter.stats\n")
            report_cmds.write("for f in OandE/FixRef.rawdata*err; do awk \'{print FILENAME, $0}\' $f | tail -q -n 40 | grep -e NS | grep -v Number | cut -d \"=\" -f 2 | awk \'{split($1,a,\".\"); print a[1],$3,$4,$5}\' | sed \'s/ref /ref-/\' | sed \'s/fixed /fixed-/\' | sed \'s/%//\'; done > accessory/FixRef.stats\n")
            report_cmds.write("zcat " + REFVCF +  " | vawk \'{split(I$CAF,a,\",\"); if($1==20) print $1,$2,$3,a[1]}\' | awk \'{if(NF==4) print $0}\' > accessory/ref_freqs.txt\n")
            report_cmds.write("for f in */*chr20.vcf.gz; do zgrep -v \"#\" $f | shuf -n 10000 |  vawk \'{print $1,$2,$3, S$*$GT}\' | tr \"/\" \" \" | tr \":\" \" \" > ${f%.vcf.gz}_freq.txt; done\n")

            report_line = f"echo \'rmarkdown::render(\"scripts/DataPrep_report.Rmd\", output_file=\"{OUT}-report.pdf\", " \
                          f"params=list(flt_file=\"accessory/filter.stats\", " \
                          f"FixRef_file=\"accessory/FixRef.stats\"," \
                          f"rulegraph_file=\"{OUT}-rulegraph.png\", " \
                          f"ref_freqs=\"accessory/ref_freqs.txt\", " \
                          f"dat_freq=\"{params.input_list}\", " \
                          f"config_file=\"workflow/config.yml\"))\' | R --vanilla"
            report_cmds.write(report_line)
        cmd = f"{CMD_PREFIX} sh scripts/gather_report_data.sh; mv scripts/{OUT}-report.pdf {OUT}-report.pdf"
        shell(cmd)

