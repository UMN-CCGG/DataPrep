#Modify all lines indicated with MODIFY
samples: "all"  # This can be set to path of file containing individuals to be kept from merged query file. One sample per line.
outname: "ALL" #Name that you want to give to any output
query: #!!ONLY USE TEXT AND NUMBERS FOR NAMING DATASETS.  List one or more datasets to be QCed via format demonstrated below.
  "dataset1":
    data: "PATH_TO_PLINK_PREFIX"
    chrom_key: "/home/pmonnaha/shared/misc/PlinkChrRename.txt" # Required.  this is also downloaded to accessory directory upon cloning repo
    allele_key: "PATH_TO_FILE_WITH_ACGT_SPECIFCATION" # Used for renaming A/B allele specification to ACGT specification (see PLINK documentation for format).  Set to 'none' if data already in ACGT
    ID_key: "PATH_TO_FILE_WITH_rsIDs" #Used to relabel markers with the rsID (see PLINK documentation for format).  Set to 'none' if rsIDs already present
    flip_key: "PATH_TO_FILE_WITH_rsIDs_TO_FLIP" #File with rsIDs to flip.  Typically retrieved from CSV file for array.  Set to 'none' if you are certain that strand is specified correctly.
    pheno_file: "PATH_TO_FILE_WITH_PHENOTYPE_INFO" #If phenotypes are already specified or are not available, set this to 'none'.  Otherwise, format according to PLINK --update-pheno.
    sex_file: "PATH_TO_FILE_WITH_SEX_INFO" #If sexes are already specified or are not available, set this to 'none'. Otherwise, format according to PLINK --update-sex.
  "dataset2":
    data: "PATH_TO_PLINK_PREFIX"
    chrom_key: "/home/pmonnaha/shared/misc/PlinkChrRename.txt" # this is also downloaded to accessory directory upon cloning repo
    allele_key: "PATH_TO_FILE_WITH_ACGT_SPECIFCATION" # Used for renaming A/B allele specification to ACGT specification (see PLINK documentation for format).  Set to 'none' if data already in ACGT
    ID_key: "PATH_TO_FILE_WITH_rsIDs" #Used to relabel markers with the rsID (see PLINK documentation for format).  Set to 'none' if rsIDs already present
    flip_key: "PATH_TO_FILE_WITH_rsIDs_TO_FLIP" #File with rsIDs to flip.  Typically retrieved from CSV file for array.  Set to 'none' if you are certain that strand is specified correctly.
    pheno_file: "PATH_TO_FILE_WITH_PHENOTYPE_INFO" #If phenotypes are already specified or are not available, set this to 'none'.  Otherwise, format according to PLINK --update-pheno.
    sex_file: "PATH_TO_FILE_WITH_SEX_INFO" #If sexes are already specified or are not available, set this to 'none'. Otherwise, format according to PLINK --update-sex.
singularity:
  image: '/home/pmonnaha/shared/singularity/AncestryInference.sif' # MODIFY: Path to singularity image file.  The one I used for everything is named AncestryInference.sif, which can be created from scratch using .def file in AncInf repo.
  module: 'module load singularity' #Command that the HPC requires for use of singularity.  Set to empty string if singularity is loaded by default.
  code: 'scripts/' #Path to the scripts directory that was cloned with repo.  Should not need to change this value.
  use_singularity: 'true' #Use a singularity image (i.e. set to true) if possible.  Pipeline has only been tested/debugged with singularity.
FixRef: 'true'
download_reference: 'true'
reference:
  refFasta: "none"
  refVCF: 'none'
perform_QC: 'true'
QC:
  vm1: "0.2"
  gm: "0.1"
  vm2: "0.05"  # Ultimate call rate for variants after removing low-callrate samples
  maf: "0.01"  # mimimum Minor allele frequency
  hwe: "0.0000001"  # p-value threshold for whether site follows hardy-weinberg
  mbs: "0.0000001"  # p-value treshold for test of whether missingness varies by sex
  mbc: "0.0001"  # p-value threshold for missingness by case. Note that this is applied to each dataset separately.  Testing subsequent to merging (if requested) must be done manually.
chroms: "all"
include_x: "no"
bcftools:
  executable: "bcftools"
  threads: '6'
  plugins: '/usr/local/bin/miniconda/pkgs/bcftools-1.9-h68d8f2e_9/libexec/bcftools/'
merge: "true"
delete_intermediates: "false"
cmd_prefix: "module load plink/1.90b6.10; module load htslib/1.6; module load bcftools/1.9;"
python: 'python'
run_settings:
  local_run: 'false'
  cluster_config: 'workflow/cluster_slurm.yaml'
  scheduler: 'slurm'

